## NOTE: Still working out some bugs to improve performance

# Using the AI Gym https://github.com/openai/gym/wiki/MountainCar-v0
# This program uses Actor-Critic Deep Reinforcement Learning to solve the Mountain Car game
# From the URL above, solving is considered an average reward of -110 over 100 continuous episodes
# The program uses a combination of Python, NumPy, and TensorFlow libraries

import gym
import numpy as np
import tensorflow as tf

# HYPERPARAMETER - Discount Factor = 0.99
gamma = 0.99
stateSz = 2
hiddenSzA = 100
hiddenSzC = 100
actionSz = 3

state= tf.placeholder(shape=[None,stateSz],dtype=tf.float32)
rewards = tf.placeholder(shape=[None],dtype=tf.float32)
actions = tf.placeholder(shape=[None],dtype=tf.int32)

# Actor NN
W =tf.Variable(tf.random_uniform([stateSz,hiddenSzA],dtype=tf.float32))
b = tf.Variable(tf.random_uniform([hiddenSzA],dtype=tf.float32))
hidden= tf.nn.relu(tf.matmul(state,W))+b
V = tf.Variable(tf.random_uniform([hiddenSzA,actionSz],dtype=tf.float32))
bV = tf.Variable(tf.random_uniform([actionSz],dtype=tf.float32))
output= tf.nn.softmax(tf.matmul(hidden,V)+bV)

# Critic NN
WCritic = tf.Variable(tf.random_normal([stateSz,hiddenSzC],dtype=tf.float32,stddev=0.1))
bCritic = tf.Variable(tf.random_normal([hiddenSzC],dtype=tf.float32,stddev=0.1))
v1Hidden = tf.nn.relu(tf.matmul(state,WCritic)+bCritic)
VCritic = tf.Variable(tf.random_normal([hiddenSzC,1],dtype=tf.float32,stddev=0.1))
bVCritic = tf.Variable(tf.random_normal([1],dtype=tf.float32,stddev=0.1))
vOut = tf.matmul(v1Hidden,VCritic) + bVCritic

# Calculating Loss
indices = tf.range(0, tf.shape(output)[0]) * actionSz + actions
actProbs = tf.gather(tf.reshape(output, [-1]), indices)
lossActor = -tf.reduce_mean(tf.multiply(tf.log(actProbs),rewards))
lossCritic = tf.reduce_mean(tf.square(rewards-vOut))
combinedLoss = lossActor + lossCritic

# Learning Rate = 0.001 w/ Adam Optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
trainOp = optimizer.minimize(combinedLoss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# Running the episodes
env = gym.make('MountainCar-v0')
total_reward = np.zeros(100) + np.inf
i = 0

while (sum(total_reward)/100) > 110:
    i += 1
    #print('Iteration {0:4} and Avg Reward: {1}'.format(i,sum(total_reward/100)))
    hist = [[],[],[],[]]
    st = env.reset()
    j = 0
    done = False
    while not done:
        j += 1
        st = [st]
        outV,action_distribution = sess.run([vOut,output],feed_dict = {state: st})		
        act = np.random.choice(2,p=action_distribution[0]) # select action randomly from actDist
        st1,reward,done,_ = env.step(act) 
        hist[0].append(st[0]) # History of the states
        hist[1].append(act) # History of the actions taken
        hist[2].append(reward) # History of the rewards from the actions
        hist[3].append(outV[0][0]) # History of the output of the critic NN
        st = st1
        # Train every 50 steps or when done
        if j%50==0 or done:
            len_hist = len(hist[2])
            dis_rs = [hist[2][len_hist - 1]]
            # Create Discounted Reward Array
            for k in range(len_hist-2,-1,-1):
                dis_rs = [dis_rs[0] * gamma + hist[2][k]] + dis_rs #-hist[3][k]
            # Subtract the critic value for each timestep from the discounted rewards
            for l in range(len_hist): dis_rs[l]-=hist[3][l]
            ploss=sess.run([comLoss,trainOp],feed_dict={state: hist[0], actions: hist[1], rewards: dis_rs})
            print ploss[0]
    total_reward[i%100] = j


print('Total Episodes: {0}'.format(i))
print('Number of Episodes until solve: {0}'.format(i-100))

